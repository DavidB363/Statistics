{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David Brookes\n",
    "# January 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Statistical Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task achieved using:\n",
    "# 1. Simple python commands\n",
    "# 2. Library software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in classification problems.\n",
    "#\n",
    "# Dependent variable is discrete. \n",
    "# Independent variable is continuous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example.\n",
    "# Assume the dependent variable is binary and discrete with labels y=0 and y=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to maximise the log likelihood function :-\n",
    "\n",
    "$ J(w,b)=\\frac{1}{N} \\sum\\limits_{i=1}^{i=N}[y_i \\log_{e}h(w,b,x_{i})+(1-y_i) \\log_{e}(1-h(w,b,x_{i})] $\n",
    "\n",
    "where\n",
    "$ h(w,b,x_{i}) = \\frac{1}{1+e^{-(w x_{i}+b)}}$ \n",
    "\n",
    "which means that\n",
    "\n",
    "$ \\frac{\\partial J}{\\partial w} = \\frac{1}{N} \\sum\\limits_{i=1}^{i=N} x_i(y_i -\\hat y_i) $\n",
    "\n",
    "$ \\frac{\\partial J}{\\partial b} = \\frac{1}{N} \\sum\\limits_{i=1}^{i=N} (y_i -\\hat y_i) $\n",
    "\n",
    "where\n",
    "$ \\hat y_i = h(w,b.x_{i}) $\n",
    "\n",
    "which can be thought of as the predicted probability of data point $x_{i}$ having the value $y=1$.\n",
    "\n",
    "$w$ and $b$ can be simply updated using the following equations\n",
    "\n",
    "$w_{new} = w_{old} + \\alpha \\frac{\\partial J}{\\partial w}$\n",
    "\n",
    "$b_{new} = b_{old} + \\alpha \\frac{\\partial J}{\\partial b}$\n",
    "\n",
    "where $\\alpha$ is a small positive constant (the learning parameter).\n",
    "\n",
    "This will ensure that the new value of $J$ will increase. (gradient ascent!).\n",
    "\n",
    "Note that more sophisticated methods exist (such as Newton's method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use class LogisticRegression from file Logistic_Regression.py in current directory.\n",
    "from Logistic_Regression import LogisticRegression\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "# X is a 2D numpy array.\n",
    "# X is a 1D numpy array.\n",
    "X, y = bc.data, bc.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of prediction for the binary variable y.\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred)/len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "regressor = LogisticRegression(lr = 0.0001, n_iters = 1000)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_predictions = regressor.predict(X_test)\n",
    "\n",
    "print('Accuracy = ', accuracy(y_test, y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Logistic Regression using the Scikit-learn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1.0, solver='lbfgs', max_iter=10000, multi_class='ovr')\n",
    "\n",
    "# Use the training data to create the model.\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions.\n",
    "y_preds = logreg.predict(X_test)\n",
    "\n",
    "print('Accuracy = ', accuracy(y_test, y_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
